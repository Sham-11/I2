{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMNDQSHStIGJRWWGaELmpc5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import SimpleRNN, LSTM, GRU, Dense, RepeatVector, TimeDistributed, Dropout\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt"],"metadata":{"id":"FMcQN33Y1P01","executionInfo":{"status":"ok","timestamp":1749695056881,"user_tz":-330,"elapsed":8,"user":{"displayName":"Pandey","userId":"04388758754067272223"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["def generate_data(n_samples, sequence_length, vocab_size):\n","    X = np.random.randint(1, vocab_size, size=(n_samples, sequence_length))\n","    Y = np.flip(X, axis=1)\n","    return X, Y\n","\n","n_samples = 10000\n","sequence_length = 10\n","vocab_size = 20\n","\n","X, Y = generate_data(n_samples, sequence_length, vocab_size)\n","X_one_hot = tf.keras.utils.to_categorical(X, num_classes=vocab_size)\n","Y_one_hot = tf.keras.utils.to_categorical(Y, num_classes=vocab_size)\n","\n","X_train, X_val, y_train, y_val = train_test_split(X_one_hot, Y_one_hot, test_size=0.2)"],"metadata":{"id":"EmYhqQ_w1TOF","executionInfo":{"status":"ok","timestamp":1749695083321,"user_tz":-330,"elapsed":45,"user":{"displayName":"Pandey","userId":"04388758754067272223"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["def build_seq2seq(cell_type='RNN'):\n","    model = Sequential()\n","    if cell_type == 'RNN':\n","        model.add(SimpleRNN(256, input_shape=(sequence_length, vocab_size)))\n","        model.add(RepeatVector(sequence_length))\n","        model.add(SimpleRNN(256, return_sequences=True))\n","    elif cell_type == 'LSTM':\n","        model.add(LSTM(256, input_shape=(sequence_length, vocab_size)))\n","        model.add(RepeatVector(sequence_length))\n","        model.add(LSTM(256, return_sequences=True))\n","    elif cell_type == 'GRU':\n","        model.add(GRU(256, input_shape=(sequence_length, vocab_size)))\n","        model.add(RepeatVector(sequence_length))\n","        model.add(GRU(256, return_sequences=True))\n","    model.add(Dropout(0.3))\n","    model.add(TimeDistributed(Dense(vocab_size, activation='softmax')))\n","    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n","    return model"],"metadata":{"id":"08yOvp3j1eSu","executionInfo":{"status":"ok","timestamp":1749695107612,"user_tz":-330,"elapsed":7,"user":{"displayName":"Pandey","userId":"04388758754067272223"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["results = {}\n","histories = {}\n","\n","for cell in ['RNN', 'LSTM', 'GRU']:\n","    print(f\"\\nTraining {cell}...\")\n","    model = build_seq2seq(cell)\n","    history = model.fit(X_train, y_train, epochs=20, batch_size=128,\n","                        validation_data=(X_val, y_val), verbose=0)\n","    loss, acc = model.evaluate(X_val, y_val, verbose=0)\n","    print(f\"{cell} Accuracy: {acc*100:.2f}%\")\n","    results[cell] = acc\n","    histories[cell] = history"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LFXdHGWc1wip","executionInfo":{"status":"ok","timestamp":1749695196532,"user_tz":-330,"elapsed":68685,"user":{"displayName":"Pandey","userId":"04388758754067272223"}},"outputId":"150094b6-2dea-4120-de40-1e849b9ec40c"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Training RNN...\n","RNN Accuracy: 99.65%\n","\n","Training LSTM...\n","LSTM Accuracy: 86.64%\n","\n","Training GRU...\n","GRU Accuracy: 80.58%\n"]}]},{"cell_type":"markdown","source":["*Analysis:*"],"metadata":{"id":"ayrN9elq_sbs"}},{"cell_type":"markdown","source":["| Model | Accuracy (%) |\n","| ----- | ------------ |\n","| RNN   | **99.65**    |\n","| LSTM  | 86.64        |\n","| GRU   | 80.58        |\n"],"metadata":{"id":"ES5G-GaS_yzu"}},{"cell_type":"markdown","source":["**Conclusion**\n","\n","In this sequence-to-sequence reversal task with short input sequences, the **Simple RNN significantly outperformed both LSTM and GRU**, achieving an impressive **99.65% validation accuracy**. This result, while counterintuitive given the usual superiority of LSTM/GRU on complex tasks, highlights that **model selection should be based on the nature and complexity of the task**.\n","\n","* Since the task involves **short-term dependencies** and is **deterministic**, the Simple RNN was sufficient and efficient.\n","* LSTM and GRU, despite being more advanced, likely **did not converge well** within the same training configuration due to their complexity and need for fine-tuning.\n","\n"],"metadata":{"id":"BPo0GscnAMi-"}}]}