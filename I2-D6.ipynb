{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"vhRofX21-ZsE","outputId":"2b583eed-23ba-4de5-858f-53794da7df80"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n","\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n","\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n","\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n","\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n","Epoch 1/60\n","422/422 - 95s - 226ms/step - accuracy: 0.7994 - loss: 1.3572 - val_accuracy: 0.8545 - val_loss: 1.1443\n","Epoch 2/60\n","422/422 - 81s - 192ms/step - accuracy: 0.8609 - loss: 1.0639 - val_accuracy: 0.8632 - val_loss: 0.9699\n","Epoch 3/60\n","422/422 - 81s - 191ms/step - accuracy: 0.8733 - loss: 0.9048 - val_accuracy: 0.8718 - val_loss: 0.8488\n","Epoch 4/60\n","422/422 - 84s - 198ms/step - accuracy: 0.8810 - loss: 0.7694 - val_accuracy: 0.8683 - val_loss: 0.7415\n","Epoch 5/60\n","422/422 - 81s - 192ms/step - accuracy: 0.8854 - loss: 0.6619 - val_accuracy: 0.8728 - val_loss: 0.6584\n","Epoch 6/60\n","422/422 - 71s - 169ms/step - accuracy: 0.8909 - loss: 0.5797 - val_accuracy: 0.8762 - val_loss: 0.5927\n","Epoch 7/60\n","422/422 - 82s - 194ms/step - accuracy: 0.8951 - loss: 0.5208 - val_accuracy: 0.8792 - val_loss: 0.5551\n","Epoch 8/60\n","422/422 - 85s - 202ms/step - accuracy: 0.8999 - loss: 0.4742 - val_accuracy: 0.8860 - val_loss: 0.5174\n","Epoch 9/60\n","422/422 - 80s - 191ms/step - accuracy: 0.9034 - loss: 0.4418 - val_accuracy: 0.8802 - val_loss: 0.5079\n","Epoch 10/60\n","422/422 - 71s - 168ms/step - accuracy: 0.9049 - loss: 0.4243 - val_accuracy: 0.8820 - val_loss: 0.5060\n","Epoch 11/60\n","422/422 - 72s - 170ms/step - accuracy: 0.9098 - loss: 0.4087 - val_accuracy: 0.8803 - val_loss: 0.4884\n","Epoch 12/60\n","422/422 - 73s - 173ms/step - accuracy: 0.9113 - loss: 0.3997 - val_accuracy: 0.8793 - val_loss: 0.4947\n","Epoch 13/60\n","422/422 - 80s - 190ms/step - accuracy: 0.9159 - loss: 0.3840 - val_accuracy: 0.8700 - val_loss: 0.5204\n","Epoch 14/60\n","422/422 - 83s - 196ms/step - accuracy: 0.9193 - loss: 0.3706 - val_accuracy: 0.8748 - val_loss: 0.4946\n","Epoch 15/60\n","422/422 - 80s - 190ms/step - accuracy: 0.9237 - loss: 0.3588 - val_accuracy: 0.8930 - val_loss: 0.4634\n","Epoch 16/60\n","422/422 - 71s - 169ms/step - accuracy: 0.9236 - loss: 0.3559 - val_accuracy: 0.8838 - val_loss: 0.4824\n","Epoch 17/60\n","422/422 - 81s - 193ms/step - accuracy: 0.9281 - loss: 0.3458 - val_accuracy: 0.8867 - val_loss: 0.4770\n","Epoch 18/60\n","422/422 - 81s - 193ms/step - accuracy: 0.9330 - loss: 0.3315 - val_accuracy: 0.8828 - val_loss: 0.4867\n","Epoch 19/60\n","422/422 - 70s - 166ms/step - accuracy: 0.9347 - loss: 0.3257 - val_accuracy: 0.8880 - val_loss: 0.4863\n","Epoch 20/60\n","422/422 - 82s - 194ms/step - accuracy: 0.9373 - loss: 0.3155 - val_accuracy: 0.8828 - val_loss: 0.4993\n","Epoch 21/60\n","422/422 - 70s - 167ms/step - accuracy: 0.9422 - loss: 0.3043 - val_accuracy: 0.8877 - val_loss: 0.5018\n","Epoch 22/60\n","422/422 - 70s - 165ms/step - accuracy: 0.9448 - loss: 0.2945 - val_accuracy: 0.8835 - val_loss: 0.5241\n","Epoch 23/60\n","422/422 - 71s - 169ms/step - accuracy: 0.9455 - loss: 0.2883 - val_accuracy: 0.8852 - val_loss: 0.5172\n","Epoch 24/60\n","422/422 - 81s - 192ms/step - accuracy: 0.9510 - loss: 0.2708 - val_accuracy: 0.8680 - val_loss: 0.5499\n","Epoch 25/60\n","422/422 - 81s - 193ms/step - accuracy: 0.9526 - loss: 0.2659 - val_accuracy: 0.8823 - val_loss: 0.5231\n","Final Test Accuracy: 89.11%\n"]}],"source":["import tensorflow as tf\n","from tensorflow.keras import layers, regularizers, Model, Input\n","from tensorflow.keras.callbacks import EarlyStopping\n","import numpy as np\n","\n","(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n","x_train = x_train.reshape(-1, 784).astype(\"float32\") / 255.0\n","x_test = x_test.reshape(-1, 784).astype(\"float32\") / 255.0\n","\n","noise = np.random.normal(0, 0.05, x_train.shape)\n","x_train_aug = np.clip(x_train + noise, 0.0, 1.0)\n","\n","# Residual block definition\n","def residual_block(x, units, dropout_rate=0.5):\n","    shortcut = x\n","    if x.shape[-1] != units:\n","        shortcut = layers.Dense(units, kernel_regularizer=regularizers.l2(1e-4))(shortcut)\n","\n","    x = layers.Dense(units, kernel_regularizer=regularizers.l2(1e-4))(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.Activation(\"relu\")(x)\n","    x = layers.Dropout(dropout_rate)(x)\n","\n","    x = layers.Dense(units, kernel_regularizer=regularizers.l2(1e-4))(x)\n","    x = layers.BatchNormalization()(x)\n","\n","    x = layers.Add()([x, shortcut])\n","    x = layers.Activation(\"relu\")(x)\n","    return x\n","\n","# Model with >10 layers using residuals\n","def create_deep_resnet():\n","    inputs = Input(shape=(784,))\n","    x = layers.BatchNormalization()(inputs)\n","\n","    x = layers.Dense(1024, kernel_regularizer=regularizers.l2(1e-4))(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.Activation(\"relu\")(x)\n","\n","    # Add 6 residual blocks with varied units\n","    x = residual_block(x, 1024, 0.4)\n","    x = residual_block(x, 512, 0.4)\n","    x = residual_block(x, 512, 0.4)\n","    x = residual_block(x, 256, 0.3)\n","    x = residual_block(x, 256, 0.3)\n","    x = residual_block(x, 128, 0.2)\n","\n","    x = layers.Dense(64, kernel_regularizer=regularizers.l2(1e-4))(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.Activation(\"relu\")(x)\n","\n","    x = layers.Dropout(0.2)(x)\n","    outputs = layers.Dense(10, activation='softmax')(x)\n","\n","    return Model(inputs=inputs, outputs=outputs)\n","\n","model = create_deep_resnet()\n","model.compile(\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0007),\n","    loss='sparse_categorical_crossentropy',\n","    metrics=['accuracy']\n",")\n","\n","early_stop = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n","history = model.fit(\n","    x_train_aug, y_train,\n","    validation_split=0.1,\n","    epochs=60,\n","    batch_size=128,\n","    callbacks=[early_stop],\n","    verbose=2\n",")\n","\n","test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n","print(f\"Final Test Accuracy: {test_acc * 100:.2f}%\")\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"P6EwnUDO_g5R","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749532723427,"user_tz":-330,"elapsed":2373380,"user":{"displayName":"Shambhavi Pandey","userId":"17695630120534427792"}},"outputId":"99f71f68-1d7e-466e-e58d-517eccb27c81"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","422/422 - 92s - 218ms/step - accuracy: 0.7991 - loss: 1.3567 - val_accuracy: 0.8548 - val_loss: 1.1295\n","Epoch 2/100\n","422/422 - 79s - 187ms/step - accuracy: 0.8589 - loss: 1.0662 - val_accuracy: 0.8577 - val_loss: 1.0151\n","Epoch 3/100\n","422/422 - 85s - 201ms/step - accuracy: 0.8722 - loss: 0.9071 - val_accuracy: 0.8602 - val_loss: 0.8846\n","Epoch 4/100\n","422/422 - 70s - 166ms/step - accuracy: 0.8816 - loss: 0.7714 - val_accuracy: 0.8745 - val_loss: 0.7288\n","Epoch 5/100\n","422/422 - 72s - 171ms/step - accuracy: 0.8878 - loss: 0.6620 - val_accuracy: 0.8783 - val_loss: 0.6556\n","Epoch 6/100\n","422/422 - 71s - 167ms/step - accuracy: 0.8911 - loss: 0.5797 - val_accuracy: 0.8713 - val_loss: 0.5969\n","Epoch 7/100\n","422/422 - 82s - 195ms/step - accuracy: 0.8957 - loss: 0.5205 - val_accuracy: 0.8733 - val_loss: 0.5895\n","Epoch 8/100\n","422/422 - 71s - 168ms/step - accuracy: 0.8989 - loss: 0.4765 - val_accuracy: 0.8772 - val_loss: 0.5160\n","Epoch 9/100\n","422/422 - 71s - 168ms/step - accuracy: 0.9052 - loss: 0.4408 - val_accuracy: 0.8768 - val_loss: 0.5099\n","Epoch 10/100\n","422/422 - 81s - 193ms/step - accuracy: 0.9054 - loss: 0.4239 - val_accuracy: 0.8765 - val_loss: 0.5030\n","Epoch 11/100\n","422/422 - 71s - 168ms/step - accuracy: 0.9102 - loss: 0.4027 - val_accuracy: 0.8798 - val_loss: 0.4863\n","Epoch 12/100\n","422/422 - 71s - 168ms/step - accuracy: 0.9126 - loss: 0.3951 - val_accuracy: 0.8702 - val_loss: 0.5137\n","Epoch 13/100\n","422/422 - 72s - 170ms/step - accuracy: 0.9168 - loss: 0.3804 - val_accuracy: 0.8778 - val_loss: 0.4956\n","Epoch 14/100\n","422/422 - 81s - 191ms/step - accuracy: 0.9200 - loss: 0.3714 - val_accuracy: 0.8855 - val_loss: 0.4838\n","Epoch 15/100\n","422/422 - 83s - 197ms/step - accuracy: 0.9230 - loss: 0.3611 - val_accuracy: 0.8850 - val_loss: 0.4810\n","Epoch 16/100\n","422/422 - 81s - 192ms/step - accuracy: 0.9244 - loss: 0.3524 - val_accuracy: 0.8807 - val_loss: 0.5107\n","Epoch 17/100\n","422/422 - 82s - 195ms/step - accuracy: 0.9309 - loss: 0.3384 - val_accuracy: 0.8837 - val_loss: 0.4903\n","Epoch 18/100\n","422/422 - 82s - 195ms/step - accuracy: 0.9313 - loss: 0.3367 - val_accuracy: 0.8838 - val_loss: 0.4994\n","Epoch 19/100\n","422/422 - 82s - 194ms/step - accuracy: 0.9346 - loss: 0.3257 - val_accuracy: 0.8810 - val_loss: 0.5162\n","Epoch 20/100\n","422/422 - 83s - 196ms/step - accuracy: 0.9374 - loss: 0.3151 - val_accuracy: 0.8862 - val_loss: 0.4859\n","Epoch 21/100\n","422/422 - 80s - 190ms/step - accuracy: 0.9413 - loss: 0.3010 - val_accuracy: 0.8827 - val_loss: 0.5033\n","Epoch 22/100\n","422/422 - 82s - 195ms/step - accuracy: 0.9436 - loss: 0.2951 - val_accuracy: 0.8813 - val_loss: 0.5053\n","Epoch 23/100\n","422/422 - 82s - 195ms/step - accuracy: 0.9491 - loss: 0.2791 - val_accuracy: 0.8855 - val_loss: 0.4998\n","Epoch 24/100\n","422/422 - 73s - 173ms/step - accuracy: 0.9497 - loss: 0.2729 - val_accuracy: 0.8827 - val_loss: 0.5208\n","Epoch 25/100\n","422/422 - 79s - 187ms/step - accuracy: 0.9526 - loss: 0.2647 - val_accuracy: 0.8838 - val_loss: 0.5298\n","Epoch 26/100\n","422/422 - 83s - 197ms/step - accuracy: 0.9546 - loss: 0.2540 - val_accuracy: 0.8845 - val_loss: 0.5133\n","Epoch 27/100\n","422/422 - 70s - 167ms/step - accuracy: 0.9581 - loss: 0.2410 - val_accuracy: 0.8797 - val_loss: 0.5408\n","Epoch 28/100\n","422/422 - 72s - 171ms/step - accuracy: 0.9586 - loss: 0.2423 - val_accuracy: 0.8785 - val_loss: 0.5497\n","Epoch 29/100\n","422/422 - 83s - 196ms/step - accuracy: 0.9608 - loss: 0.2332 - val_accuracy: 0.8742 - val_loss: 0.5495\n","Epoch 30/100\n","422/422 - 80s - 190ms/step - accuracy: 0.9633 - loss: 0.2297 - val_accuracy: 0.8787 - val_loss: 0.5547\n","Final Test Accuracy: 88.36%\n"]}],"source":["import tensorflow as tf\n","from tensorflow.keras import layers, regularizers, Model, Input\n","from tensorflow.keras.callbacks import EarlyStopping\n","import numpy as np\n","\n","(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n","x_train = x_train.reshape(-1, 784).astype(\"float32\") / 255.0\n","x_test = x_test.reshape(-1, 784).astype(\"float32\") / 255.0\n","\n","noise = np.random.normal(0, 0.05, x_train.shape)\n","x_train_aug = np.clip(x_train + noise, 0.0, 1.0)\n","\n","def residual_block(x, units, dropout_rate=0.5):\n","    shortcut = x\n","    if x.shape[-1] != units:\n","        shortcut = layers.Dense(units, kernel_regularizer=regularizers.l2(1e-4))(shortcut)\n","\n","    x = layers.Dense(units, kernel_regularizer=regularizers.l2(1e-4))(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.Activation(\"relu\")(x)\n","    x = layers.Dropout(dropout_rate)(x)\n","\n","    x = layers.Dense(units, kernel_regularizer=regularizers.l2(1e-4))(x)\n","    x = layers.BatchNormalization()(x)\n","\n","    x = layers.Add()([x, shortcut])\n","    x = layers.Activation(\"relu\")(x)\n","    return x\n","\n","# Model with >10 layers using residuals\n","def create_deep_resnet():\n","    inputs = Input(shape=(784,))\n","    x = layers.BatchNormalization()(inputs)\n","\n","    x = layers.Dense(1024, kernel_regularizer=regularizers.l2(1e-4))(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.Activation(\"relu\")(x)\n","\n","    # Add 6 residual blocks with varied units\n","    x = residual_block(x, 1024, 0.4)\n","    x = residual_block(x, 512, 0.4)\n","    x = residual_block(x, 512, 0.4)\n","    x = residual_block(x, 256, 0.3)\n","    x = residual_block(x, 256, 0.3)\n","    x = residual_block(x, 128, 0.2)\n","\n","    x = layers.Dense(64, kernel_regularizer=regularizers.l2(1e-4))(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.Activation(\"relu\")(x)\n","\n","    x = layers.Dropout(0.2)(x)\n","    outputs = layers.Dense(10, activation='softmax')(x)\n","\n","    return Model(inputs=inputs, outputs=outputs)\n","\n","model = create_deep_resnet()\n","model.compile(\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0007),\n","    loss='sparse_categorical_crossentropy',\n","    metrics=['accuracy']\n",")\n","\n","early_stop = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n","history = model.fit(\n","    x_train_aug, y_train,\n","    validation_split=0.1,\n","    epochs=100,\n","    batch_size=128,\n","    callbacks=[early_stop],\n","    verbose=2\n",")\n","\n","test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n","print(f\"Final Test Accuracy: {test_acc * 100:.2f}%\")\n"]},{"cell_type":"markdown","source":["**Reference Model: 87-88%,   Model 1: 89.11%,   Model 2: 88.36%**\n","\n"],"metadata":{"id":"2c1WVbCtiNj3"}},{"cell_type":"markdown","source":["*Conclusion:*\n","\n","1.   The above 2 models have deeper architecture with over 10 layers, including 6 residual blocks than the Reference code.\n","2.   Residual blocks help prevent vanishing gradients and allow deeper models to train effectively.\n","1.   This model includes L2 regularization, dropout, and batch normalization for better generalization.\n","2.   Data augmentation using Gaussian noise makes the model more robust to input variations.\n","1.   EarlyStopping prevents overfitting by stopping training when validation accuracy stops improving.\n","2.   A smaller learning rate with the Adam optimizer ensures stable and smooth convergence.\n","1.   The combination of depth, regularization, augmentation, and residuals leads to improved accuracy (Model 1: 89.11% and Model 2: 88.36%) over Reference model (~87–88%).\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"Hrg1UiwVeAk9"}}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNmiWg3wFpLbtG0e3he60rS"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}